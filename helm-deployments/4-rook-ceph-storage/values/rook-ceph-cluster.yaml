rook-ceph-cluster:
  enabled: true
  operatorNamespace: rook-ceph-storage
  clusterName:
  cephClusterSpec:
    # This cluster spec example is for a converged cluster where all the Ceph daemons are running locally,
    # as in the host-based example (cluster.yaml). For a different configuration such as a
    # PVC-based cluster (cluster-on-pvc.yaml), external cluster (cluster-external.yaml),
    # or stretch cluster (cluster-stretched.yaml), replace this entire `cephClusterSpec`
    # with the specs from those examples.

    # For more details, check https://rook.io/docs/rook/v1.10/CRDs/Cluster/ceph-cluster-crd/
    cephVersion:
      # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
      # v18 is Reef, v19 is Squid
      # RECOMMENDATION: In production, use a specific version tag instead of the general v18 flag, which pulls the latest release and could result in different
      # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
      # If you want to be more precise, you can always use a timestamp tag such as quay.io/ceph/ceph:v19.2.0-20240927
      # This tag might not contain a new Ceph version, just security fixes from the underlying operating system, which will reduce vulnerabilities
      image: quay.io/ceph/ceph:v19.2.0
      # Whether to allow unsupported versions of Ceph. Currently Reef and Squid are supported.
      # Future versions such as Tentacle (v20) would require this to be set to `true`.
      # Do not set to true in production.
      allowUnsupported: false

    # The path on the host where configuration files will be persisted. Must be specified. If there are multiple clusters, the directory must be unique for each cluster.
    # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
    # In Minikube, the '/data' directory is configured to persist across reboots. Use "/data/rook" in Minikube environment.
    dataDirHostPath: /var/lib/rook

    # Whether or not upgrade should continue even if a check fails
    # This means Ceph's status could be degraded and we don't recommend upgrading but you might decide otherwise
    # Use at your OWN risk
    # To understand Rook's upgrade process of Ceph, read https://rook.io/docs/rook/v1.10/Upgrade/ceph-upgrade/
    skipUpgradeChecks: false

    # Whether or not continue if PGs are not clean during an upgrade
    continueUpgradeAfterChecksEvenIfNotHealthy: false

    # WaitTimeoutForHealthyOSDInMinutes defines the time (in minutes) the operator would wait before an OSD can be stopped for upgrade or restart.
    # If the timeout exceeds and OSD is not ok to stop, then the operator would skip upgrade for the current OSD and proceed with the next one
    # if `continueUpgradeAfterChecksEvenIfNotHealthy` is `false`. If `continueUpgradeAfterChecksEvenIfNotHealthy` is `true`, then operator would
    # continue with the upgrade of an OSD even if its not ok to stop after the timeout. This timeout won't be applied if `skipUpgradeChecks` is `true`.
    # The default wait timeout is 10 minutes.
    waitTimeoutForHealthyOSDInMinutes: 10

    # Whether or not requires PGs are clean before an OSD upgrade. If set to `true` OSD upgrade process won't start until PGs are healthy.
    # This configuration will be ignored if `skipUpgradeChecks` is `true`.
    # Default is false.
    upgradeOSDRequiresHealthyPGs: false

    mon:
      # Set the number of mons to be started. Generally recommended to be 3.
      # For highest availability, an odd number of mons should be specified.
      count: 3
      # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
      # Mons should only be allowed on the same node for test environments where data loss is acceptable.
      allowMultiplePerNode: false

    mgr:
      # When higher availability of the mgr is needed, increase the count to 2.
      # In that case, one mgr will be active and one in standby. When Ceph updates which
      # mgr is active, Rook will update the mgr services to match the active mgr.
      count: 2
      allowMultiplePerNode: false
      modules:
        # List of modules to optionally enable or disable.
        # Note the "dashboard" and "monitoring" modules are already configured by other settings in the cluster CR.
        # - name: rook
        #   enabled: true

    # enable the ceph dashboard for viewing cluster status
    dashboard:
      enabled: true
      # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
      # urlPrefix: /ceph-dashboard
      # serve the dashboard at the given port.
      # port: 8443
      # Serve the dashboard using SSL (if using ingress to expose the dashboard and `ssl: true` you need to set
      # the corresponding "backend protocol" annotation(s) for your ingress controller of choice)
      ssl: true

    # Network configuration, see: https://github.com/rook/rook/blob/v1.16.3/Documentation/CRDs/Cluster/ceph-cluster-crd.md#network-configuration-settings
    network:
      connections:
        # Whether to encrypt the data in transit across the wire to prevent eavesdropping the data on the network.
        # The default is false. When encryption is enabled, all communication between clients and Ceph daemons, or between Ceph daemons will be encrypted.
        # When encryption is not enabled, clients still establish a strong initial authentication and data integrity is still validated with a crc check.
        # IMPORTANT: Encryption requires the 5.11 kernel for the latest nbd and cephfs drivers. Alternatively for testing only,
        # you can set the "mounter: rbd-nbd" in the rbd storage class, or "mounter: fuse" in the cephfs storage class.
        # The nbd and fuse drivers are *not* recommended in production since restarting the csi driver pod will disconnect the volumes.
        encryption:
          enabled: false
        # Whether to compress the data in transit across the wire. The default is false.
        # The kernel requirements above for encryption also apply to compression.
        compression:
          enabled: false
        # Whether to require communication over msgr2. If true, the msgr v1 port (6789) will be disabled
        # and clients will be required to connect to the Ceph cluster with the v2 port (3300).
        # Requires a kernel that supports msgr v2 (kernel 5.11 or CentOS 8.4 or newer).
        requireMsgr2: false
    #   # enable host networking
    #   provider: host
    #   # EXPERIMENTAL: enable the Multus network provider
    #   provider: multus
    #   selectors:
    #     # The selector keys are required to be `public` and `cluster`.
    #     # Based on the configuration, the operator will do the following:
    #     #   1. if only the `public` selector key is specified both public_network and cluster_network Ceph settings will listen on that interface
    #     #   2. if both `public` and `cluster` selector keys are specified the first one will point to 'public_network' flag and the second one to 'cluster_network'
    #     #
    #     # In order to work, each selector value must match a NetworkAttachmentDefinition object in Multus
    #     #
    #     # public: public-conf --> NetworkAttachmentDefinition object name in Multus
    #     # cluster: cluster-conf --> NetworkAttachmentDefinition object name in Multus
    #   # Provide internet protocol version. IPv6, IPv4 or empty string are valid options. Empty string would mean IPv4
    #   ipFamily: "IPv6"
    #   # Ceph daemons to listen on both IPv4 and Ipv6 networks
    #   dualStack: false

    # enable the crash collector for ceph daemon crash collection
    crashCollector:
      disable: false
      # Uncomment daysToRetain to prune ceph crash entries older than the
      # specified number of days.
      # daysToRetain: 30

    # enable log collector, daemons will log on files and rotate
    logCollector:
      enabled: true
      periodicity: daily # one of: hourly, daily, weekly, monthly
      maxLogSize: 500M # SUFFIX may be 'M' or 'G'. Must be at least 1M.

    # automate [data cleanup process](https://github.com/rook/rook/blob/v1.16.3/Documentation/Storage-Configuration/ceph-teardown.md#delete-the-data-on-hosts) in cluster destruction.
    cleanupPolicy:
      # Since cluster cleanup is destructive to data, confirmation is required.
      # To destroy all Rook data on hosts during uninstall, confirmation must be set to "yes-really-destroy-data".
      # This value should only be set when the cluster is about to be deleted. After the confirmation is set,
      # Rook will immediately stop configuring the cluster and only wait for the delete command.
      # If the empty string is set, Rook will not destroy any data on hosts during uninstall.
      confirmation: ""
      # sanitizeDisks represents settings for sanitizing OSD disks on cluster deletion
      sanitizeDisks:
        # method indicates if the entire disk should be sanitized or simply ceph's metadata
        # in both case, re-install is possible
        # possible choices are 'complete' or 'quick' (default)
        method: quick
        # dataSource indicate where to get random bytes from to write on the disk
        # possible choices are 'zero' (default) or 'random'
        # using random sources will consume entropy from the system and will take much more time then the zero source
        dataSource: zero
        # iteration overwrite N times instead of the default (1)
        # takes an integer value
        iteration: 1
      # allowUninstallWithVolumes defines how the uninstall should be performed
      # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
      allowUninstallWithVolumes: false

    # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
    # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
    # tolerate taints with a key of 'storage-node'.
    # placement:
    #   all:
    #     nodeAffinity:
    #       requiredDuringSchedulingIgnoredDuringExecution:
    #         nodeSelectorTerms:
    #           - matchExpressions:
    #             - key: role
    #               operator: In
    #               values:
    #               - storage-node
    #     podAffinity:
    #     podAntiAffinity:
    #     topologySpreadConstraints:
    #     tolerations:
    #     - key: storage-node
    #       operator: Exists
    #   # The above placement information can also be specified for mon, osd, and mgr components
    #   mon:
    #   # Monitor deployments may contain an anti-affinity rule for avoiding monitor
    #   # collocation on the same node. This is a required rule when host network is used
    #   # or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
    #   # preferred rule with weight: 50.
    #   osd:
    #   mgr:
    #   cleanup:

    storage: # cluster level storage configuration and selection
      useAllNodes: true
      useAllDevices: true
      # deviceFilter:
      # config:
      #   crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
      #   metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
      #   databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
      #   osdsPerDevice: "1" # this value can be overridden at the node or device level
      #   encryptedDevice: "true" # the default value for this option is "false"
      # # Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
      # # nodes below will be used as storage resources. Each node's 'name' field should match their 'kubernetes.io/hostname' label.
      # nodes:
      #   - name: "172.17.4.201"
      #     devices: # specific devices to use for storage can be specified for each node
      #       - name: "sdb"
      #       - name: "nvme01" # multiple osds can be created on high performance devices
      #         config:
      #           osdsPerDevice: "5"
      #       - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
      #     config: # configuration can be specified at the node level which overrides the cluster level config
      #   - name: "172.17.4.301"
      #     deviceFilter: "^sd."

    # The section for configuring management of daemon disruptions during upgrade or fencing.
    disruptionManagement:
      # If true, the operator will create and manage PodDisruptionBudgets for OSD, Mon, RGW, and MDS daemons. OSD PDBs are managed dynamically
      # via the strategy outlined in the [design](https://github.com/rook/rook/blob/v1.16.3/design/ceph/ceph-managed-disruptionbudgets.md). The operator will
      # block eviction of OSDs by default and unblock them safely when drains are detected.
      managePodBudgets: true
      # A duration in minutes that determines how long an entire failureDomain like `region/zone/host` will be held in `noout` (in addition to the
      # default DOWN/OUT interval) when it is draining. This is only relevant when  `managePodBudgets` is `true`. The default value is `30` minutes.
      osdMaintenanceTimeout: 30
      # A duration in minutes that the operator will wait for the placement groups to become healthy (active+clean) after a drain was completed and OSDs came back up.
      # Operator will continue with the next drain if the timeout exceeds. It only works if `managePodBudgets` is `true`.
      # No values or 0 means that the operator will wait until the placement groups are healthy before unblocking the next drain.
      pgHealthCheckTimeout: 0

  ingress:
    dashboard:
      host:
        name: rook-ceph.q.t
        path: "/ceph-dashboard(/|$)(.*)"
      ingressClassName: core-services-traefik

  # -- A list of CephBlockPool configurations to deploy
  # @default -- See [below](#ceph-block-pools)
  cephBlockPools:
    - name: ceph-blockpool
      # see https://github.com/rook/rook/blob/v1.16.3/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
      spec:
        failureDomain: host
        replicated:
          size: 3
        # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
        # For reference: https://docs.ceph.com/docs/latest/mgr/prometheus/#rbd-io-statistics
        # enableRBDStats: true
      storageClass:
        enabled: true
        name: ceph-block
        isDefault: false
  cephFileSystems:
    - name: ceph-filesystem
      # see https://github.com/rook/rook/blob/v1.16.3/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
          - failureDomain: host
            replicated:
              size: 3
            # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/v1.16.3/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
            name: data0
        metadataServer:
          activeCount: 1
          activeStandby: true
          resources:
            limits:
              memory: "4Gi"
            requests:
              cpu: "1000m"
              memory: "4Gi"
          priorityClassName: system-cluster-critical
      storageClass:
        enabled: true
        isDefault: false
        name: ceph-filesystem
        pool: data0

  # -- Settings for the filesystem snapshot class
  # @default -- See [CephFS Snapshots](../Storage-Configuration/Ceph-CSI/ceph-csi-snapshot.md#cephfs-snapshots)
  cephFileSystemVolumeSnapshotClass:
    enabled: false
    name: ceph-filesystem
    isDefault: true
    deletionPolicy: Delete
    annotations: {}
    labels: {}
    # see https://rook.io/docs/rook/v1.10/Storage-Configuration/Ceph-CSI/ceph-csi-snapshot/#cephfs-snapshots for available configuration
    parameters: {}

  # -- Settings for the block pool snapshot class
  # @default -- See [RBD Snapshots](../Storage-Configuration/Ceph-CSI/ceph-csi-snapshot.md#rbd-snapshots)
  cephBlockPoolsVolumeSnapshotClass:
    enabled: false
    name: ceph-block
    isDefault: false
    deletionPolicy: Delete
    annotations: {}
    labels: {}
    # see https://rook.io/docs/rook/v1.10/Storage-Configuration/Ceph-CSI/ceph-csi-snapshot/#rbd-snapshots for available configuration
    parameters: {}

  # -- A list of CephObjectStore configurations to deploy
  # @default -- See [below](#ceph-object-stores)
  cephObjectStores:
    - name: ceph-objectstore
      # see https://github.com/rook/rook/blob/v1.16.3/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
          parameters:
            bulk: "true"
        preservePoolsOnDelete: true
        gateway:
          port: 80
      storageClass:
        enabled: true
        name: ceph-bucket
        parameters:
          region: vn
      ingress:
        # Enable an ingress for the ceph-objectstore
        enabled: false
        # annotations: {}
        # host:
        #   name: objectstore.example.com
        #   path: /
        # tls:
        # - hosts:
        #     - objectstore.example.com
        #   secretName: ceph-objectstore-tls
        # ingressClassName: nginx
  